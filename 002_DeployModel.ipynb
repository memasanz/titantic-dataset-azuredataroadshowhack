{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "- There are several ways to deploy a model.  This is a same notebook walking you through the process of "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "import pandas as pd\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Python environment for the experiment (from a .yml file)\n",
        "# Import required packages\n",
        "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset, Model\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import Webservice, AciWebservice\n",
        "from azureml.exceptions import WebserviceException\n",
        "\n",
        "conda_yml_file = './'+ experiment_folder+ '/environment.yml'\n",
        "env = Environment.from_conda_specification(\"experiment_env\", conda_yml_file)\n",
        "env.register(workspace=ws)\n",
        "print('')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'titanic-model'\n",
        "environment_name = 'experiment_env'\n",
        "service_name = 'servicename'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Register the model\n",
        "## Prepare entry script\n",
        "## Prepare inference configration\n",
        "## deploy model\n",
        "## test results"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create a folder for the pipeline step files\n",
        "experiment_folder = 'prep_for_deploy'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "aci_config = AciWebservice.deploy_configuration(\n",
        "            cpu_cores = 1, \n",
        "            memory_gb = 2, \n",
        "            tags = {'model': 'diabetes remote training'},\n",
        "            auth_enabled=True,\n",
        "            enable_app_insights=True,\n",
        "            collect_model_data=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Inference Config"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./$experiment_folder/environment.yml\n",
        "\n",
        "name: experiment_env\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- scikit-learn\n",
        "- ipykernel\n",
        "- matplotlib\n",
        "- pandas=1.1.5\n",
        "- pip\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - pyarrow\n",
        "  - azureml-monitoring\n",
        "  - azureml-interpret\n",
        "  - inference-schema\n",
        "  - joblib\n",
        "  - azure-ml-api-sdk\n",
        "  - seaborn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=\"./image/Capture.png\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes on preparing for API Call\n",
        "\n",
        "- Manually grab url & api key, you can get them programmically & you will later in the notebook\n",
        "- Be familiar with where in teh UI they are located."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEPLOY ME: Update the init script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./$experiment_folder/score.py\n",
        "\n",
        "import os \n",
        "import json\n",
        "import joblib\n",
        "from pandas import json_normalize\n",
        "import pandas as pd\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # Replace filename if needed.\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'titanic_model.pkl')\n",
        "    # Deserialize the model file back into a sklearn model.\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "def run(data):\n",
        "    dict= json.loads(data)\n",
        "    df = json_normalize(dict['data']) \n",
        "    df['loc']= df['cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
        "    df['hasFamily'] = (df['sibsp'] > 0) | (df['parch'] > 0)\n",
        "    #note we remove the survived column\n",
        "    cols_to_keep = ['pclass','sex','age','embarked','loc','hasFamily']\n",
        "    df = df[cols_to_keep]\n",
        "    \n",
        "    print(df.isnull().sum())\n",
        "    y_pred = model.predict(df)\n",
        "    print(type(y_pred))\n",
        "    \n",
        "    #return json.dumps(y_pred)\n",
        "    result = {\"result\": y_pred.tolist()}\n",
        "    return result"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./prep_for_deploy/score.py\n"
        }
      ],
      "execution_count": 48,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(ws, model_name)\n",
        "env = Environment.get(ws, environment_name)\n",
        "inference_config = InferenceConfig(entry_script='./prep_for_deploy/score.py', environment=env)\n",
        "\n",
        "try:\n",
        "    service = Webservice(ws, name=service_name)\n",
        "    if service:\n",
        "        service.delete()\n",
        "except WebserviceException as e:\n",
        "         print()\n",
        "\n",
        "service = Model.deploy(ws, service_name, [model], inference_config, aci_config)\n",
        "service.wait_for_deployment(True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nTips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2022-02-28 16:08:55+00:00 Creating Container Registry if not exists.\n2022-02-28 16:08:55+00:00 Registering the environment.\n2022-02-28 16:08:56+00:00 Building image..\n2022-02-28 16:14:24+00:00 Generating deployment configuration.\n2022-02-28 16:14:25+00:00 Submitting deployment to compute..\n2022-02-28 16:14:29+00:00 Checking the status of deployment servicename..\n2022-02-28 16:17:44+00:00 Checking the status of inference endpoint servicename.\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\n"
        }
      ],
      "execution_count": 50,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(service.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Healthy\n"
        }
      ],
      "execution_count": 51,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = service.scoring_uri\n",
        "print(endpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "http://b6c30ea8-240c-4d91-90cf-e3b18f3cb736.eastus.azurecontainer.io/score\n"
        }
      ],
      "execution_count": 52,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "keys = service.get_keys()\n",
        "selected_key = keys[0]\n",
        "print(selected_key)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ETRP33uJdZaKjDaogKgjUEBXJIShst1O\n"
        }
      ],
      "execution_count": 53,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "url = endpoint\n",
        "api_key = selected_key # Replace this with the API key for the web service\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
        "import requests\n",
        "\n",
        "def MakePrediction(df):\n",
        "    if 'survived' in df:\n",
        "        print('removing surived from dataset')\n",
        "        df.drop(['survived'], axis=1, inplace=True)\n",
        "    endpoint_url = url\n",
        "    body = df.to_json(orient='records') \n",
        "    body = '{\"data\": ' + body + '}'\n",
        "    r = requests.post(endpoint_url, headers=headers, data=body)\n",
        "    return (r.json())\n",
        "\n",
        "\n",
        "df = pd.read_csv('./Data/Hold.csv')\n",
        "results = MakePrediction(df)\n",
        "print(type(results))\n",
        "\n",
        "val = results['result']\n",
        "print(val)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "removing surived from dataset\n<class 'dict'>\n[0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n"
        }
      ],
      "execution_count": 57,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./Data/Hold.csv')\n",
        "df.columns"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "Index(['passenger_id', 'pclass', 'name', 'sex', 'age', 'sibsp', 'parch',\n       'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest',\n       'survived'],\n      dtype='object')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_test  =  df['survived'].tolist()\n",
        "y_pred  = val\n",
        "results = pd.DataFrame({'y_test': y_test, 'y_pred': val})\n",
        "results['y_test'] = results['y_test'].astype(int)\n",
        "results"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_test</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>393 rows Ã— 2 columns</p>\n</div>",
            "text/plain": "     y_test  y_pred\n0         0       0\n1         1       1\n2         0       0\n3         1       1\n4         0       0\n..      ...     ...\n388       0       0\n389       1       1\n390       0       1\n391       0       0\n392       1       1\n\n[393 rows x 2 columns]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 75,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(acc)\n",
        "print(prec)\n",
        "print(recall)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.7837150127226463\n0.7310344827586207\n0.6973684210526315\n"
        }
      ],
      "execution_count": 76,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "experiment_env",
      "language": "python",
      "display_name": "experiment_env"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "experiment_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}