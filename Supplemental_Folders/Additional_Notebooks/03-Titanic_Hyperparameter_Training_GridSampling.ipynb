{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train Model with Titanic Dataset using Hyperdrive experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to your workspace\n",
    "\n",
    "Let's get started by connecting to the AML workspace leveraging the Azure ML SDK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.38.0 to work with mm-hackathon-prep\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "import pandas as pd\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dataset\n",
    "\n",
    "You're going to use a Python script to train a machine learning model based on the Titanic datset found in your data folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 6)\n",
      "(917, 8)\n",
      "(917, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.1500</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211.3375</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare  embarked  survived  pclass  sex   age  sibsp  parch  loc\n",
       "0    8.0500         2       0.0     3.0    1  24.0    0.0    0.0    8\n",
       "1   21.0000         2       0.0     2.0    1  43.0    0.0    1.0    8\n",
       "2   24.1500         2       0.0     3.0    0  10.0    0.0    2.0    8\n",
       "3   15.5000         1       0.0     3.0    1  24.0    0.0    0.0    8\n",
       "4  211.3375         2       1.0     1.0    0  43.0    0.0    1.0    1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('./Data/Train1.csv')\n",
    "df2 = pd.read_csv('./Data/Train2.csv')\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "df = df1.merge(df2, on = 'passenger_id', how = 'inner')\n",
    "\n",
    "df['survived'] = df['survived'].fillna(0)\n",
    "df['loc']= df['cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df['age'] = df.groupby(['pclass'])['age'].apply(lambda x: x.fillna(x.median()))\n",
    "df = df.drop(['name', 'ticket', 'home.dest', 'cabin', 'passenger_id'], axis = 1)\n",
    "\n",
    "\n",
    "df_features = list(df.columns)\n",
    "df_features.remove(\"survived\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['embarked'] = df['embarked'].fillna('S')\n",
    "df['embarked'] = label_encoder.fit_transform(df['embarked'])\n",
    "df['sex'] = label_encoder.fit_transform(df['sex'])\n",
    "df['loc'] = label_encoder.fit_transform(df['loc'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz4/code/Users/memasanz/titantic-dataset-private/Supplemental_Folders/Additional_Notebooks/hyperparameter_train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"hyperparameter_train\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fare  embarked  survived  pclass  sex   age  sibsp  parch  loc\n",
       "0   8.05         2       0.0     3.0    1  24.0    0.0    0.0    8\n",
       "1  21.00         2       0.0     2.0    1  43.0    0.0    1.0    8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('./hyperparameter_train/titanic.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./hyperparameter_train/titanic.csv\n",
      "Uploaded ./hyperparameter_train/titanic.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Dataset registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "#use default datastore retrieved from the workspace through the AML SDK\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "default_ds.upload_files(files=['./hyperparameter_train/titanic.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path= 'hyper-drive-data', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "#Create a tabular dataset from the path on the datastore \n",
    "dataset = Dataset.Tabular.from_delimited_files(default_ds.path('hyper-drive-data/titanic.csv'))\n",
    "\n",
    "# Register the dataset\n",
    "try:\n",
    "    tab_data_set = dataset.register(workspace=ws, \n",
    "                                name= 'hyper-drive-data-titanic-tabular-dataset',\n",
    "                                description='Tintanic data',\n",
    "                                tags = {'format':'csv'},\n",
    "                                create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "        print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, 'titanic_hyperdrive_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Script\n",
    "\n",
    "Create a folder to hold your training script.  By running the magic command **%%writefile%%** a file will be generated and placed into the **$script_folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz4/code/Users/memasanz/titantic-dataset-private/Supplemental_Folders/Additional_Notebooks/hyperparameter_train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"hyperparameter_train\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz4/code/Users/memasanz/titantic-dataset-private/Supplemental_Folders/Additional_Notebooks/hyperparameter_train/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/training.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Run, Dataset, Workspace, Experiment\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "# Calculate model performance metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "def getRuntimeArgs():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str)\n",
    "    parser.add_argument(\"--penalty_term\", type=str, dest = 'penalty_term', default = 'l2', help = 'penalty term')\n",
    "    parser.add_argument('--C', type=float, dest='C', default=0.1, help='learning rate')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def buildpreprocessorpipeline(X_raw):\n",
    "    categorical_features = X_raw.select_dtypes(include=['object']).columns\n",
    "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=\"missing\")),\n",
    "                                              ('onehotencoder', OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric', numeric_transformer, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)\n",
    "        ], remainder=\"drop\")\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def model_train(LABEL, df, run, penalty_term, C_value):  \n",
    "    y_raw = df[LABEL]\n",
    "    X_raw = df.drop([LABEL], axis=1)\n",
    "    \n",
    "     # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=0)\n",
    "    \n",
    "    lg = LogisticRegression(penalty=penalty_term, C=C_value, solver='liblinear')\n",
    "    preprocessor = buildpreprocessorpipeline(X_train)\n",
    "    \n",
    "    #estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lg)])\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    run.log('AUC', np.float(auc))\n",
    "\n",
    "    \n",
    "    # calculate test accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "    run.log('Accuracy', np.float(acc))\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    run.log_image(name = \"ROC\", plot = fig)\n",
    "    plt.show()\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # Generate confusion matrix\n",
    "    cmatrix = confusion_matrix(y_test, y_hat)\n",
    "    cmatrix_json = {\n",
    "        \"schema_type\": \"confusion_matrix\",\n",
    "           \"schema_version\": \"v1\",\n",
    "           \"data\": {\n",
    "               \"class_labels\": [\"0\", \"1\"],\n",
    "               \"matrix\": [\n",
    "                   [int(x) for x in cmatrix[0]],\n",
    "                   [int(x) for x in cmatrix[1]]\n",
    "               ]\n",
    "           }\n",
    "    }\n",
    "    \n",
    "    run.log_confusion_matrix('ConfusionMatrix_Test', cmatrix_json)\n",
    "\n",
    "    return model, auc, acc\n",
    "    # Save the trained model\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # Create an Azure ML experiment in your workspace\n",
    "    args = getRuntimeArgs()\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    run.log('penalty_term', args.penalty_term)\n",
    "    run.log('C', np.float(args.C))\n",
    "    dataset_dir = './dataset/'\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    ws = run.experiment.workspace\n",
    "    print(ws)\n",
    "    \n",
    "\n",
    "    print(\"Loading Data...\")\n",
    "    #dataset = Dataset.get_by_id(ws, id=args.input_data)\n",
    "    dataset = run.input_datasets['titanic']\n",
    "    # Load a TabularDataset & save into pandas DataFrame\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    print(df.head(5))\n",
    " \n",
    "    model, auc, acc = model_train('Survived', df, run, args.penalty_term, args.C)\n",
    "    \n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    \n",
    "    model_file = os.path.join('outputs', 'titanic_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    \n",
    "    # Register the model\n",
    "    print('Registering model...')\n",
    "    Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'titanic_model',\n",
    "               tags={'Training context':'Compute'},\n",
    "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "    run.complete()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz4/code/Users/memasanz/titantic-dataset-private/Supplemental_Folders/Additional_Notebooks/hyperparameter_train/experiment_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/experiment_env.yml\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment from the conda specification file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment-env defined.\n",
      "name: experiment_env\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "- scikit-learn\n",
      "- ipykernel\n",
      "- matplotlib\n",
      "- pandas\n",
      "- pip\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "  - pyarrow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification('experiment-env', script_folder + \"/experiment_env.yml\")\n",
    "\n",
    "# Let Azure ML manage dependencies\n",
    "experiment_env.python.user_managed_dependencies = False \n",
    "\n",
    "# Print the environment details\n",
    "print(experiment_env.name, 'defined.')\n",
    "print(experiment_env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a compute cluster\n",
    "\n",
    "Previously, the model was trained on a compute instance, by creating a compute cluster, we can submit the job to the compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"aml-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec8ea2950344f2795a4010195f3b7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/HD_b7b63a65-68dd-4130-a5c5-47584a33141b?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-hackathon-prep-rg/workspaces/mm-hackathon-prep&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\", \"run_id\": \"HD_b7b63a65-68dd-4130-a5c5-47584a33141b\", \"run_properties\": {\"run_id\": \"HD_b7b63a65-68dd-4130-a5c5-47584a33141b\", \"created_utc\": \"2022-03-16T06:09:46.779779Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"AUC\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"d14d6027-5f05-4bff-b87f-0020d2d4e28e\", \"user_agent\": \"python/3.6.9 (Linux-5.4.0-1068-azure-x86_64-with-debian-buster-sid) msrest/0.6.21 Hyperdrive.Service/1.0.0 Hyperdrive.SDK/core.1.38.0\", \"space_size\": \"8\"}, \"tags\": {\"_aml_system_max_concurrent_jobs\": \"2\", \"_aml_system_max_total_jobs\": \"8\", \"_aml_system_max_duration_minutes\": \"10080\", \"_aml_system_policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"_aml_system_generator_config\": \"{\\\"name\\\": \\\"GRID\\\", \\\"parameter_space\\\": {\\\"--penalty_term\\\": [\\\"choice\\\", [[\\\"l2\\\", \\\"l1\\\"]]], \\\"--C\\\": [\\\"choice\\\", [[0.01, 0.1, 1, 10]]]}}\", \"_aml_system_primary_metric_config\": \"{\\\"name\\\": \\\"AUC\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"_aml_system_platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://eastus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-hackathon-prep-rg/providers/Microsoft.MachineLearningServices/workspaces/mm-hackathon-prep/experiments/titanic-hyperdrive\\\", \\\"SubscriptionId\\\": \\\"5da07161-3770-4a4b-aa43-418cbbb627cf\\\", \\\"ResourceGroupName\\\": \\\"mm-hackathon-prep-rg\\\", \\\"WorkspaceName\\\": \\\"mm-hackathon-prep\\\", \\\"ExperimentName\\\": \\\"titanic-hyperdrive\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"training.py\\\", \\\"arguments\\\": [\\\"--input-data\\\", \\\"DatasetConsumptionConfig:titanic\\\"], \\\"target\\\": \\\"aml-cluster\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": 2592000, \\\"nodeCount\\\": 1, \\\"priority\\\": null, \\\"environment\\\": {\\\"name\\\": \\\"experiment-env\\\", \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"experiment_env\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", \\\"scikit-learn\\\", \\\"ipykernel\\\", \\\"matplotlib\\\", \\\"pandas\\\", \\\"pip\\\", {\\\"pip\\\": [\\\"azureml-defaults\\\", \\\"pyarrow\\\"]}]}}, \\\"docker\\\": {\\\"enabled\\\": false, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220113.v1\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}, \\\"platform\\\": {\\\"os\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\"}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"docker\\\": {\\\"useDocker\\\": false, \\\"sharedVolumes\\\": true, \\\"arguments\\\": [], \\\"shmSize\\\": \\\"2g\\\"}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1, \\\"nodeCount\\\": 1}, \\\"pytorch\\\": {\\\"communicationBackend\\\": \\\"nccl\\\", \\\"processCount\\\": null, \\\"nodeCount\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {}, \\\"data\\\": {\\\"titanic\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"4edc6d35-a69c-4df7-957b-9a6c3c047357\\\", \\\"name\\\": \\\"hyper-drive-data-titanic-tabular-dataset\\\", \\\"version\\\": 1}, \\\"dataPath\\\": null, \\\"uri\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"direct\\\", \\\"environmentVariableName\\\": \\\"titanic\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false, \\\"options\\\": null}}, \\\"datacaches\\\": [], \\\"outputData\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": null}, \\\"kubernetescompute\\\": {\\\"instanceType\\\": null}, \\\"credentialPassthrough\\\": false, \\\"command\\\": \\\"\\\", \\\"environmentVariables\\\": {}, \\\"applicationEndpoints\\\": {}}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"d14d6027-5f05-4bff-b87f-0020d2d4e28e\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\", \\\"amlClientRequestId\\\": \\\"2a59f89a-2feb-44a3-90c4-94a6e0e08f77\\\", \\\"amlClientSessionId\\\": \\\"ba887732-acc7-43d2-b25e-ada4d22f75f0\\\", \\\"subscriptionId\\\": \\\"5da07161-3770-4a4b-aa43-418cbbb627cf\\\", \\\"estimator\\\": \\\"NoneType\\\", \\\"samplingMethod\\\": \\\"GRID\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 8, \\\"maxConcurrentRuns\\\": 2, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"_aml_system_resume_child_runs\": \"null\", \"_aml_system_all_jobs_generated\": \"false\", \"_aml_system_cancellation_requested\": \"false\", \"_aml_system_progress_metadata_evaluation_timestamp\": \"\\\"2022-03-16T06:09:48.177668\\\"\", \"_aml_system_progress_metadata_digest\": \"\\\"9869172aaaf5c03ba7ae1f869d6c267f33087ed702246eab675e21066214c76d\\\"\", \"_aml_system_progress_metadata_active_timestamp\": \"\\\"2022-03-16T06:09:48.177668\\\"\", \"_aml_system_optimizer_state_artifact\": \"null\", \"_aml_system_outdated_optimizer_state_artifacts\": \"\\\"[]\\\"\", \"_aml_system_HD_b7b63a65-68dd-4130-a5c5-47584a33141b_0\": \"{\\\"--penalty_term\\\": \\\"l1\\\", \\\"--C\\\": 0.01}\", \"_aml_system_HD_b7b63a65-68dd-4130-a5c5-47584a33141b_1\": \"{\\\"--penalty_term\\\": \\\"l1\\\", \\\"--C\\\": 0.1}\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://mmhackathonpre4120422243.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_b7b63a65-68dd-4130-a5c5-47584a33141b/azureml-logs/hyperdrive.txt?sv=2019-07-07&sr=b&sig=1g6LQCrj3cctGqcbm6RSOFBCLr6bbpz6JgNMmHgdGwc%3D&skoid=f676a930-738c-4b9f-82a1-19a06ad4180e&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-03-16T03%3A54%3A28Z&ske=2022-03-17T12%3A04%3A28Z&sks=b&skv=2019-07-07&st=2022-03-16T06%3A00%3A32Z&se=2022-03-16T14%3A10%3A32Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:02:10\", \"run_number\": \"1647410986\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}, \"hyper_parameters\": {\"--penalty_term\": [\"choice\", [[\"l2\", \"l1\"]]], \"--C\": [\"choice\", [[0.01, 0.1, 1, 10]]]}}, \"child_runs\": [{\"run_id\": \"HD_b7b63a65-68dd-4130-a5c5-47584a33141b_1\", \"run_number\": 1647411018, \"metric\": null, \"status\": \"Finalizing\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2022-03-16T06:11:25.313118Z\", \"end_time\": \"\", \"created_time\": \"2022-03-16T06:10:18.10964Z\", \"created_time_dt\": \"2022-03-16T06:10:18.10964Z\", \"duration\": \"0:01:39\", \"hyperdrive_id\": \"b7b63a65-68dd-4130-a5c5-47584a33141b\", \"arguments\": null, \"param_--penalty_term\": \"l1\", \"param_--C\": 0.1}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2022-03-16T06:09:47.089264][API][INFO]Experiment created\\r\\n[2022-03-16T06:09:47.879213][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\\r\\n[2022-03-16T06:09:48.443325][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\\r\\n[2022-03-16T06:10:17.5104949Z][SCHEDULER][INFO]Scheduling job, id='HD_b7b63a65-68dd-4130-a5c5-47584a33141b_0'\\r\\n[2022-03-16T06:10:17.5118329Z][SCHEDULER][INFO]Scheduling job, id='HD_b7b63a65-68dd-4130-a5c5-47584a33141b_1'\\r\\n[2022-03-16T06:10:18.1281117Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_b7b63a65-68dd-4130-a5c5-47584a33141b_0'\\r\\n[2022-03-16T06:10:18.1991379Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_b7b63a65-68dd-4130-a5c5-47584a33141b_1'\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.38.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import azureml.core.runconfig\n",
    "from azureml.core import Environment, Experiment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Get the training dataset\n",
    "titanic_ds = ws.datasets.get('Titanic-tabular-dataset')\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=script_folder,\n",
    "                                script='training.py',\n",
    "                                arguments=['--input-data', tab_data_set.as_named_input('titanic')], # Reference to dataset\n",
    "                                environment=experiment_env,\n",
    "                                compute_target=cluster_name)\n",
    "\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        # Hyperdrive will try 8 combinations, adding these as script arguments\n",
    "        '--penalty_term': choice('l2', 'l1'),\n",
    "        '--C': choice(.01, .1, 1,10)\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=None, # No early stopping policy\n",
    "                          primary_metric_name='AUC', # Find the highest AUC metric\n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=8, # Restict the experiment to 6 iterations\n",
    "                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='titanic-hyperdrive')\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "RunDetails(run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steady 1\n"
     ]
    }
   ],
   "source": [
    "cluster_state = training_cluster.get_status()\n",
    "print(cluster_state.allocation_state, cluster_state.current_node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printchild_run = 0\n",
    "if printchild_run == 1:\n",
    "    print(\"child run information\")\n",
    "    for child_run in run.get_children_sorted_by_primary_metric():\n",
    "        print(child_run)\n",
    "\n",
    "# Get the best run\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "#get the metrics for the best run\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "\n",
    "#review the metics for the best run\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC:', best_run_metrics['AUC'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_run.register_model(model_path='outputs/titanic_model.pkl', model_name='titanic_model',\n",
    "                        tags={'Training context':'Hyperdrive'},\n",
    "                        properties={'AUC': best_run_metrics['AUC'], 'Accuracy': best_run_metrics['Accuracy']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
